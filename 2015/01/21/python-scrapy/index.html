
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论         | enjoyhot</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="enjoyhot">
    

    
    <meta name="description" content="一、综述  开始这篇博文之前，调研了相关的爬虫方法，简单罗列冰山一角。
  综述：
http://www.crifan.com/summary_about_flow_process_of_fetch_webpage_simulate_login_website_and_some_notice/">
<meta property="og:type" content="article">
<meta property="og:title" content="网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论        ">
<meta property="og:url" content="http://www.enjoyhot.top/2015/01/21/python-scrapy/index.html">
<meta property="og:site_name" content="enjoyhot">
<meta property="og:description" content="一、综述  开始这篇博文之前，调研了相关的爬虫方法，简单罗列冰山一角。
  综述：
http://www.crifan.com/summary_about_flow_process_of_fetch_webpage_simulate_login_website_and_some_notice/">
<meta property="og:image" content="http://img.blog.csdn.net/20150121152425667?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20150121163439093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20150121163529857?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20150121163516500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20150121171359378?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论        ">
<meta name="twitter:description" content="一、综述  开始这篇博文之前，调研了相关的爬虫方法，简单罗列冰山一角。
  综述：
http://www.crifan.com/summary_about_flow_process_of_fetch_webpage_simulate_login_website_and_some_notice/">
<link rel="publisher" href="111736291216286617217">

    
    <link rel="alternative" href="/atom.xml" title="enjoyhot" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="enjoyhot" title="enjoyhot"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="enjoyhot">enjoyhot</a></h1>
				<h2 class="blog-motto">All things come to those who wait.</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home|主页</a></li>
					
						<li><a href="/archives">Archives|归档</a></li>
					
						<li><a href="/about">About|关于</a></li>
					
					<li>
 					
						<form class="search" action="http://deepso.enjoyhot.top/" target="_blank">
							<label>Search</label>
						<input name="s" type="hidden" value= 12369960100802390000 ><input type="text" name="q" size="30" placeholder="搜索"><br>
						</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2015/01/21/python-scrapy/" title="网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论        " itemprop="url">网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论        </a>
  </h1>
  <p class="article-author">By
       
		<a href="https://plus.google.com/111736291216286617217?rel=author" title="enjoyhot" target="_blank" itemprop="author">enjoyhot</a>
		
  <p class="article-time">
    <time datetime="2015-01-21T07:12:00.000Z" itemprop="datePublished"> 发表于 2015-01-21</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、综述"><span class="toc-number">1.</span> <span class="toc-text">一、综述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、scrapy框架"><span class="toc-number">2.</span> <span class="toc-text">二、scrapy框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、整体架构如下："><span class="toc-number">2.1.</span> <span class="toc-text">1、整体架构如下：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、工程文件介绍"><span class="toc-number">2.2.</span> <span class="toc-text">2、工程文件介绍</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、新浪新闻爬虫"><span class="toc-number">3.</span> <span class="toc-text">三、新浪新闻爬虫</span></a></li></ol>
		
		</div>
		
		<h1 id="一、综述">一、综述</h1><p>  开始这篇博文之前，调研了相关的爬虫方法，简单罗列冰山一角。</p>
<p>  综述：</p>
<p><a href="http://www.crifan.com/summary_about_flow_process_of_fetch_webpage_simulate_login_website_and_some_notice/" target="_blank" rel="external">http://www.crifan.com/summary_about_flow_process_of_fetch_webpage_simulate_login_website_and_some_notice/</a></p>
 <a id="more"></a>
<p>  手动编写爬虫，httpclient是常用工具。常见的请求方式有httpget和httppost</p>
<p><a href="http://blog.csdn.net/mr_tank_/article/details/17454315" target="_blank" rel="external">http://blog.csdn.net/mr_tank_/article/details/17454315</a></p>
<p><a href="http://blog.csdn.net/chszs/article/details/16854747" target="_blank" rel="external">http://blog.csdn.net/chszs/article/details/16854747</a></p>
<p><a href="http://www.yeetrack.com/?p=779" target="_blank" rel="external">http://www.yeetrack.com/?p=779</a></p>
<p>  这个教程很全面。供参考和备查</p>
<p>  htmlunit</p>
<p>  httpclient 对js 的支持比较差，有时候需要使用htmlunit 或者selenium。</p>
<p><a href="http://www.360doc.com/content/13/1229/14/14875906_340995211.shtml" target="_blank" rel="external">http://www.360doc.com/content/13/1229/14/14875906_340995211.shtml</a></p>
<p><a href="http://blog.csdn.net/strawbingo/article/details/5768421" target="_blank" rel="external">http://blog.csdn.net/strawbingo/article/details/5768421</a></p>
<p><a href="http://www.cnblogs.com/microsoftmvp/p/3716750.html" target="_blank" rel="external">http://www.cnblogs.com/microsoftmvp/p/3716750.html</a></p>
<p> 抽取相关<br>当爬取了html 后，需要去除噪声广告，抽取有用的信息。jsoup 和tika 是非常强大的工具</p>
<p><a href="http://jsoup.org/cookbook/" target="_blank" rel="external">http://jsoup.org/cookbook/</a></p>
<p><a href="http://summerbell.iteye.com/blog/565922" target="_blank" rel="external">http://summerbell.iteye.com/blog/565922</a></p>
<p>  github开源爬虫库</p>
<p><a href="https://github.com/CrawlScript/WebCollector" target="_blank" rel="external">https://github.com/CrawlScript/WebCollector</a></p>
<p><a href="https://github.com/zhuoran/crawler4j" target="_blank" rel="external">https://github.com/zhuoran/crawler4j</a></p>
<p>  开源爬虫框架nutch</p>
<p><a href="http://www.cnblogs.com/xuekyo/archive/2013/04/18/3028559.html" target="_blank" rel="external">http://www.cnblogs.com/xuekyo/archive/2013/04/18/3028559.html</a></p>
<p><a href="http://ahei.info/nutch-tutorial.htm" target="_blank" rel="external">http://ahei.info/nutch-tutorial.htm</a></p>
<p><a href="http://lc87624.iteye.com/blog/1625677" target="_blank" rel="external">http://lc87624.iteye.com/blog/1625677</a></p>
<p>  由于要学习python语言，就关注了python爬虫的方法，scrapy框架是个成熟的开源爬虫框架，因此选择其作为学习内容。<br>Scrapy是一个基于Twisted，纯Python实现的爬虫框架，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容、图片、视频等，非常方便。</p>
<h1 id="二、scrapy框架">二、scrapy框架</h1><h2 id="1、整体架构如下：">1、整体架构如下：</h2><p>   <img src="http://img.blog.csdn.net/20150121152425667?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image_mark"></p>
<p> 绿线是数据流向，首先从初始URL 开始，Scheduler 会将其交给 Downloader 进行下载，下载之后会交给 Spider 进行分析，Spider分析出来的结果有两种：一种是需要进一步抓取的链接，</p>
<p>例如之前分析的“下一页”的链接，这些东西会被传回 Scheduler ；另一种是需要保存的数据，它们则被送到Item Pipeline 那里，那是对数据进行后期处理（详细分析、过滤、存储等）的</p>
<p>地方。另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。参考<a href="http://blog.csdn.net/HanTangSongMing/article/details/24454453" target="_blank" rel="external"><br>    博客
   </a></p>
<h2 id="2、工程文件介绍">2、工程文件介绍</h2><p>假设你已经配置好环境了，进入某个文件夹pythonproject，在命令行中输入<br>scrapy startproject mypro<br>即可在pythonporoject文件夹下找到mypro的工程文件夹，结构如下：</p>
<p>├── mypro<br>│   ├── mypro<br>│   │   ├── <strong>init</strong>.py<br>│   │   ├── items.py<br>│   │   ├── pipelines.py<br>│   │   ├── settings.py<br>│   │   └── spiders<br>│   │      └── <strong>init</strong>.py<br>│   └── scrapy.cfg</p>
<p>scrapy.cfg: 项目配置文件<br>items.py: 需要提取的数据结构定义文件<br>pipelines.py:管道定义，用来对items里面提取的数据做进一步处理，如保存等<br>settings.py: 爬虫配置文件</p>
<p>Items是将要装载抓取的数据的容器，它工作方式像python里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误。它通过创建一个scrapy.item.Item类来声明<br>，定义它的属性为scrpiy.item.Field对象，就像是一个对象关系映射(ORM)，我们通过将需要的item模型化，来控制从dmoz.org获得的站点数据。虽然这次的实现并没有用到items.py和</p>
<p>pipelines.py，但大规模的爬虫还是需要注意一下解耦。<br>举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.item <span class="keyword">import</span> Item, Field    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozItem</span><span class="params">(Item)</span>:</span>   </span><br><span class="line">    title = Field()   </span><br><span class="line">    link = Field()   </span><br><span class="line">    desc = Field()</span><br></pre></td></tr></table></figure>
<p> 在修改初始化代码时，首先需要在pythonproject//mypro//mypro//spiders下新建一个python文件，原则上所有的实现可以在这个文件里完成，当然耦合度就高了。在这个文件中，你需要新</p>
<p>建一个类，这个类需要添加以下属性：<br>1、该类继承于某个spider类，根据自己的需求，有很多可以选，如crawSpider，BaseSpider，Spider，XMLFeedSpider，CSVFeedSpider，SitemapSpider等等<br>2、name：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字，例如下文的”yourname”<br>3、start_urls：爬虫开始爬的一个URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些URLS开始。其他子URL将会从这些起始URL中继承性生成。<br>4、parse()：爬虫的方法，调用时候传入从每一个URL传回的Response对象作为参数，response将会是parse方法的唯一的一个参数，这个方法负责解析返回的数据、匹配抓取的数据(解析为</p>
<p>item)并跟踪更多的URL。返回前可以巧妙地运用yield方法递归调用网址，此关键词的作用是返回某个对象后继续执行。如果不用该关键字，则直接会在函数中返回。</p>
<p>一般而言，运用scrapy的步骤是这样的：<br>1、在pythonproject//mypro//mypro//spiders下新建一个python文件<br>2、导入该导入的库文件，新建一个类满足以上要求。<br>3、根据继承的类的要求和功能，定义爬取规则。<br>4、在def parse(self, response)函数中对response对象解析，将需要的内容存入item对象并返回，在这里对数据不返回而是进行进一步处理也是可以的，耦合度高。<br>5、PipeLine用来对Spider返回的Item列表进行保存操作，可以写入到文件、或者数据库等。PipeLine只有一个需要实现的方法：process_item。<br>万事具备之后，通过命令行进入pythonproject//mypro文件夹中，敲下命令行开始爬虫<br>scrapy crawl “yourname”<br>scrapy命令罗列几个，要更多请参看<a href="http://scrapy-chs.readthedocs.org/zh_CN/0.24/topics/commands.html" target="_blank" rel="external">doc</a></p>
<ul>
<li><code>scrapy startproject xxx</code> 新建一个xxx的project</li>
<li><code>scrapy crawl xxx</code> 开始爬取，必须在project中</li>
<li><code>scrapy shell url</code> 在scrapy的shell中打开url，非常实用</li>
<li><code>scrapy runspider &lt;spider_file.py&gt;</code> 可以在没有project的情况下运行爬虫</li>
</ul>
<h1 id="三、新浪新闻爬虫">三、新浪新闻爬虫</h1><p>众所周知，评论一般是隐藏起来的，或者显示部分，需要手动点击加载去获取更多评论。有两种方法可以解决这种方法，一种是利用js动态解析，工作量大，也比较难实现，二是直接定位到其查询数据库的url，直接抽取。下文就是讲第二种方法。<br>新浪页面导航为我们简单分好类了<a href="http://news.sina.com.cn/guide/" target="_blank" rel="external">http://news.sina.com.cn/guide/</a>，而且每个类别中都可以找到相应的滚动新闻（url冠以roll），因而没必要用到crawSpider这个类，这个类功能很强大，不仅可以自动去重，还可以定义更多的爬取规则。例如这个链接<a href="http://roll.finance.sina.com.cn/finance/zq1/index_1.shtml" target="_blank" rel="external">http://roll.finance.sina.com.cn/finance/zq1/index_1.shtml</a>，通过修改数字可以实现不断爬取对于新闻的url，当然没有这么“好”的url也是可以找到新闻的url。例如：<a href="http://sports.sina.com.cn/nba/" target="_blank" rel="external">http://sports.sina.com.cn/nba/</a><br>可以调用的浏览器的开发工具查找对应的js代码，查看数据库的url，之后在查看评论的时候也是这样的方法（点击刷新即可）</p>
<p><img src="http://img.blog.csdn.net/20150121163439093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image_mark"></p>
<p><img src="http://img.blog.csdn.net/20150121163529857?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image_mark"></p>
<p> 访问这个<a href="http://feed.mix.sina.com.cn/api/roll/tags?channelid=6&amp;sq=x_where:digit_cl==399872&amp;begin=1401552000&amp;tags=%E6%B9%96%E4%BA%BA%2C%E9%AA%91%E5%A3%AB%2C%E7%81%AB%E7%AE%AD%2C%E8%A9%B9%E5%A7%86%E6%96%AF%2C%E7%A7%91%E6%AF%94&amp;num=30&amp;lid=-3000&amp;versionNumber=1.2.4&amp;page=4&amp;encode=utf-8&amp;callback=feedCardJsonpCallback&amp;_=1421828921159" target="_blank" rel="external">链接</a>可以查看url</p>
<p><img src="http://img.blog.csdn.net/20150121163516500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image_mark"></p>
<p>   因此，访问这个<a href="http://roll.finance.sina.com.cn/finance/zq1/index_1.shtml" target="_blank" rel="external">链接</a>的内容，爬取新闻url,访问新闻并爬取标题、内容、评论。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin/env python  </span></span><br><span class="line"><span class="comment">#coding=utf-8  </span></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector  </span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request  </span><br><span class="line"><span class="keyword">import</span> re,os  </span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line"><span class="keyword">from</span> scrapy.spider <span class="keyword">import</span> Spider  </span><br><span class="line"><span class="keyword">import</span> urllib2,thread  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#处理编码问题  </span></span><br><span class="line"><span class="keyword">import</span> sys  </span><br><span class="line">reload(sys)  </span><br><span class="line">sys.setdefaultencoding(<span class="string">'gb18030'</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#flag的作用是保证第一次爬取的时候不进行单个新闻页面内容的爬取  </span></span><br><span class="line">flag=<span class="number">1</span>  </span><br><span class="line">projectpath=<span class="string">'F:\\Python27\\pythonproject\\fuck\\'</span>  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">(*response)</span>:</span>  </span><br><span class="line">        sel = Selector(response[<span class="number">0</span>])   </span><br><span class="line">        <span class="comment">#get title           </span></span><br><span class="line">        title = sel.xpath(<span class="string">'//h1/text()'</span>).extract()  </span><br><span class="line">        <span class="comment">#get pages  </span></span><br><span class="line">        pages=sel.xpath(<span class="string">'//div[@id="artibody"]//p/text()'</span>).extract()  </span><br><span class="line">        <span class="comment">#get chanel_id &amp; comment_id  </span></span><br><span class="line">        s=sel.xpath(<span class="string">'//meta[@name="comment"]'</span>).extract()                  </span><br><span class="line">              </span><br><span class="line">        <span class="comment">#comment_id = channel[index+3:index+15]  </span></span><br><span class="line">        index2=len(response[<span class="number">0</span>].url)  </span><br><span class="line">        news_id=response[<span class="number">0</span>].url[index2-<span class="number">14</span>:index2-<span class="number">6</span>]  </span><br><span class="line">        comment_id=<span class="string">'31-1-'</span>+news_id      </span><br><span class="line">          </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">        <span class="comment">#评论内容都在这个list中  </span></span><br><span class="line">        cmntlist=[]  </span><br><span class="line">          </span><br><span class="line">        page=<span class="number">1</span>  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">#含有新闻url,标题,内容,评论的文件  </span></span><br><span class="line">        file2=<span class="keyword">None</span>     </span><br><span class="line">          </span><br><span class="line">        <span class="comment">#该变量的作用是当某新闻下存在非手机用户评论时置为False  </span></span><br><span class="line">        is_all_tel=<span class="keyword">True</span>  </span><br><span class="line">          </span><br><span class="line">        <span class="keyword">while</span>((page==<span class="number">1</span>) <span class="keyword">or</span> (cmntlist != [])):  </span><br><span class="line">              </span><br><span class="line">            tel_count=<span class="number">0</span> <span class="comment">#each page tel_user_count  </span></span><br><span class="line">            <span class="comment">#提取到的评论url  </span></span><br><span class="line">            url=<span class="string">"http://comment5.news.sina.com.cn/page/info?version=1&amp;format=js&amp;channel=cj&amp;newsid="</span>+str(comment_id)+<span class="string">"&amp;group=0&amp;compress=1&amp;ie=gbk&amp;oe=gbk&amp;page="</span>+str</span><br><span class="line"></span><br><span class="line">(page)+<span class="string">"&amp;page_size=100"</span>  </span><br><span class="line">            url_contain=urllib2.urlopen(url).read()  </span><br><span class="line">       </span><br><span class="line">                  </span><br><span class="line">            b=<span class="string">'=&#123;'</span>  </span><br><span class="line">            after = url_contain[url_contain.index(b)+len(b)-<span class="number">1</span>:]  </span><br><span class="line">            <span class="comment">#字符串中的None对应python中的null，不然执行eval时会出错  </span></span><br><span class="line">            after=after.replace(<span class="string">'null'</span>,<span class="string">'None'</span>)  </span><br><span class="line">            <span class="comment">#转换为字典变量text  </span></span><br><span class="line">            text=eval(after)  </span><br><span class="line">              </span><br><span class="line">            <span class="keyword">if</span> <span class="string">'cmntlist'</span> <span class="keyword">in</span> text[<span class="string">'result'</span>]:  </span><br><span class="line">                cmntlist=text[<span class="string">'result'</span>][<span class="string">'cmntlist'</span>]  </span><br><span class="line">            <span class="keyword">else</span>:  </span><br><span class="line">                cmntlist=[]                          </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">            <span class="keyword">if</span> cmntlist != [] <span class="keyword">and</span> (page==<span class="number">1</span>):  </span><br><span class="line">                filename=str(comment_id)+<span class="string">'.txt'</span>  </span><br><span class="line">                  </span><br><span class="line">                path=projectpath+<span class="string">'stock\\'</span> +filename  </span><br><span class="line">                file2=open(path,<span class="string">'a+'</span>)  </span><br><span class="line">                news_content=str(<span class="string">''</span>)  </span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> pages:                                                              </span><br><span class="line">                    news_content=news_content+p+<span class="string">'\n'</span>  </span><br><span class="line">                item=<span class="string">"&lt;url&gt;"</span>+response[<span class="number">0</span>].url+<span class="string">"&lt;/url&gt;"</span>+<span class="string">'\n\n'</span>+<span class="string">"&lt;title&gt;"</span>+str(title[<span class="number">0</span>])+<span class="string">"&lt;/title&gt;\n\n"</span>+<span class="string">"&lt;content&gt;\n"</span>+str(news_content)+<span class="string">"&lt;/content&gt;\n\n&lt;comment&gt;\n"</span>  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">                file2.write(item)  </span><br><span class="line">            <span class="keyword">if</span> cmntlist != []:  </span><br><span class="line">                content=<span class="string">''</span>  </span><br><span class="line">                  </span><br><span class="line">                <span class="keyword">for</span> status_dic <span class="keyword">in</span> cmntlist:  </span><br><span class="line">                      </span><br><span class="line">                    <span class="keyword">if</span> status_dic[<span class="string">'uid'</span>]!=<span class="string">'0'</span>:  </span><br><span class="line">                                                  </span><br><span class="line">                        is_all_tel=<span class="keyword">False</span>  </span><br><span class="line">                          </span><br><span class="line">                        <span class="comment">#这一句视编码情况而定，在这里去掉decode和encode也行  </span></span><br><span class="line">                        s=status_dic[<span class="string">'content'</span>].decode(<span class="string">'UTF-8'</span>).encode(<span class="string">'GBK'</span>)  </span><br><span class="line">                          </span><br><span class="line">                        <span class="comment">#见另一篇博客“三张图”  </span></span><br><span class="line">                        s=s.replace(<span class="string">"'"</span><span class="string">"'"</span>,<span class="string">'"'</span>) </span><br><span class="line">                        s=s.replace(<span class="string">"\n"</span>,<span class="string">''</span>) </span><br><span class="line">                        s1=<span class="string">"u'"</span>+s+<span class="string">"'"</span>  </span><br><span class="line">                        <span class="keyword">try</span>:                          </span><br><span class="line">                            ss=eval(s1)                   </span><br><span class="line">                        <span class="keyword">except</span>:  </span><br><span class="line">                            <span class="keyword">try</span>:  </span><br><span class="line">                                s1=<span class="string">'u"'</span>+s+<span class="string">'"'</span>  </span><br><span class="line">                                ss=eval(s1)  </span><br><span class="line">                            <span class="keyword">except</span>:                            </span><br><span class="line">                                <span class="keyword">return</span>  </span><br><span class="line">                          </span><br><span class="line">                          </span><br><span class="line">                        content=content+status_dic[<span class="string">'time'</span>]+<span class="string">'\t'</span>+status_dic[<span class="string">'uid'</span>]+<span class="string">'\t'</span>+ss+<span class="string">'\n'</span>  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">                    <span class="comment">#当属于手机用户时  </span></span><br><span class="line">                    <span class="keyword">else</span>:  </span><br><span class="line">                        tel_count=tel_count+<span class="number">1</span>     </span><br><span class="line">                                       </span><br><span class="line">                <span class="comment">#当一个page下不都是手机用户时，这里也可以用is_all_tel进行判断，一种是用开关的方式，一种是统计的方式  </span></span><br><span class="line">                <span class="comment">#算了不改了  </span></span><br><span class="line">                <span class="keyword">if</span> tel_count!=len(cmntlist):  </span><br><span class="line">                    file2.write(content)  </span><br><span class="line">                      </span><br><span class="line">            page=page+<span class="number">1</span>  </span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">        <span class="comment">#while loop end here  </span></span><br><span class="line">          </span><br><span class="line">        <span class="keyword">if</span> file2!=<span class="keyword">None</span>:     </span><br><span class="line">            <span class="comment">#当都是手机用户时，移除文件，否则写入"&lt;/comment&gt;"到文件尾           </span></span><br><span class="line">            <span class="keyword">if</span> is_all_tel:  </span><br><span class="line">                file2.close()  </span><br><span class="line">                <span class="keyword">try</span>:  </span><br><span class="line">                    os.remove(file2.name)  </span><br><span class="line">                <span class="keyword">except</span> WindowsError:  </span><br><span class="line">                    <span class="keyword">pass</span>  </span><br><span class="line">            <span class="keyword">else</span>:  </span><br><span class="line">                file2.write(<span class="string">"&lt;/comment&gt;"</span>)  </span><br><span class="line">                file2.close()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(Spider)</span>:</span>  </span><br><span class="line">    name = <span class="string">"stock"</span>  </span><br><span class="line">    allowed_domains = [<span class="string">"sina.com.cn"</span>]  </span><br><span class="line">     </span><br><span class="line">    <span class="comment">#在本程序中，start_urls并不重要，因为并没有解析  </span></span><br><span class="line">    start_urls = [  </span><br><span class="line">        <span class="string">"http://news.sina.com.cn/"</span>  </span><br><span class="line">    ]  </span><br><span class="line">      </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">global</span> projectpath  </span><br><span class="line">      </span><br><span class="line">    <span class="keyword">if</span> os.path.exists(projectpath+<span class="string">'stock'</span>):  </span><br><span class="line">        <span class="keyword">pass</span>  </span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        os.mkdir(projectpath+<span class="string">'stock'</span>)  </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span>  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">#这个scrapy.selector.Selector是个不错的处理字符串的类，python对编码很严格，它却处理得很好  </span></span><br><span class="line">        <span class="comment">#在做这个爬虫的时候，碰到很多奇奇怪怪的编码问题，主要是中文，试过很多既有的类，BeautifulSoup处理得也不是很好  </span></span><br><span class="line">        sel = Selector(response)            </span><br><span class="line">          </span><br><span class="line">        <span class="keyword">global</span> flag  </span><br><span class="line">                              </span><br><span class="line">        <span class="keyword">if</span>(flag==<span class="number">1</span>):  </span><br><span class="line">            flag=<span class="number">2</span>  </span><br><span class="line">            page=<span class="number">1</span>  </span><br><span class="line">            <span class="keyword">while</span> page&lt;<span class="number">260</span>:   </span><br><span class="line">                  </span><br><span class="line">                url=<span class="string">"http://roll.finance.sina.com.cn/finance/zq1/index_"</span>  </span><br><span class="line">                  </span><br><span class="line">                url=url+str(page)+<span class="string">".shtml"</span>  </span><br><span class="line">                  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">                <span class="comment">#伪装为浏览器  </span></span><br><span class="line">                user_agent = <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>    </span><br><span class="line">                headers = &#123; <span class="string">'User-Agent'</span> : user_agent &#125;                    </span><br><span class="line">                req = urllib2.Request(url, headers=headers)  </span><br><span class="line">                response = urllib2.urlopen(req)    </span><br><span class="line">                url_contain = response.read()   </span><br><span class="line">                  </span><br><span class="line">                <span class="comment">#利用BeautifulSoup进行文档解析    </span></span><br><span class="line">                soup = BeautifulSoup(url_contain)                  </span><br><span class="line">                params = soup.findAll(<span class="string">'div'</span>,&#123;<span class="string">'class'</span>:<span class="string">'listBlk'</span>&#125;)  </span><br><span class="line">                  </span><br><span class="line">                  </span><br><span class="line">                <span class="keyword">if</span> os.path.exists(projectpath+<span class="string">'stock\\'</span>+<span class="string">'link'</span>):  </span><br><span class="line">                     <span class="keyword">pass</span>  </span><br><span class="line">                <span class="keyword">else</span>:  </span><br><span class="line">                     os.mkdir(projectpath+<span class="string">'stock\\'</span>+<span class="string">'link'</span>)  </span><br><span class="line">                   </span><br><span class="line">                filename=<span class="string">'link.txt'</span>  </span><br><span class="line">                  </span><br><span class="line">                path=projectpath+<span class="string">'stock\\link\\'</span> + filename  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">                filelink=open(path,<span class="string">'a+'</span>)  </span><br><span class="line">                  </span><br><span class="line">                  </span><br><span class="line">                <span class="keyword">for</span> params_item <span class="keyword">in</span> params:          </span><br><span class="line">                    persons = params_item.findAll(<span class="string">'li'</span>)                      </span><br><span class="line">                    <span class="keyword">for</span> item <span class="keyword">in</span> persons:                      </span><br><span class="line">                        href=item.find(<span class="string">'a'</span>)  </span><br><span class="line">                        mil_link= href.get(<span class="string">'href'</span>)                                                     </span><br><span class="line">                        filelink.write(str(mil_link)+<span class="string">'\n'</span>)                                                   </span><br><span class="line">                        <span class="comment">#递归调用parse,传入新的爬取url  </span></span><br><span class="line">                        <span class="keyword">yield</span> Request(mil_link, callback=self.parse)                                </span><br><span class="line">                      </span><br><span class="line">                                      </span><br><span class="line">                page=page+<span class="number">1</span>     </span><br><span class="line">                  </span><br><span class="line">        <span class="comment">#对单个新闻页面新建线程进行爬取  </span></span><br><span class="line">        <span class="keyword">if</span> flag!=<span class="number">1</span>:  </span><br><span class="line">            <span class="keyword">if</span> (response.status != <span class="number">404</span>) <span class="keyword">and</span> (response.status != <span class="number">502</span>):  </span><br><span class="line">                thread.start_new_thread(loop,(response,))</span><br></pre></td></tr></table></figure>
<p> 爬取结果：</p>
<p><img src="http://img.blog.csdn.net/20150121171359378?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VndWd1amlhd2Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="image_mark"></p>
<p>在爬取的过程中要注意三点：<br>1.爬取不要过于频繁，不然可能会被封ip，可以减小爬取的速度，sleep一下，或者更改设置文件，我的在F:\Python27\python\Lib\site-packages\Scrapy-0.24.4-py2.7.egg\scrapy\settings\default_settings.py<br>2.文件夹的文件上限为21845，超过后注意再新建一个文件夹爬取<br>3.线程不能开得太多，不然也可能达到上限，可以考虑用代码现在所开线程的多少或者利用分布式系统</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/python/">python</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/python/">python</a><a href="/tags/scrapy/">scrapy</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://www.enjoyhot.top/2015/01/21/python-scrapy/" data-title="网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论         | enjoyhot" data-tsina="2428685001" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2015/02/25/machinelearning-KNN/" title="【machine learning】KNN算法        ">
  <strong>上一篇：</strong><br/>
  <span>
  【machine learning】KNN算法        </span>
</a>
</div>


<div class="next">
<a href="/2015/01/21/machinelearning-linear-regularization/"  title="【machine learning】regularization        ">
 <strong>下一篇：</strong><br/> 
 <span>【machine learning】regularization        
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2015/01/21/python-scrapy/" data-title="网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论        " data-url="http://www.enjoyhot.top/2015/01/21/python-scrapy/"></div>
</section>


</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、综述"><span class="toc-number">1.</span> <span class="toc-text">一、综述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、scrapy框架"><span class="toc-number">2.</span> <span class="toc-text">二、scrapy框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、整体架构如下："><span class="toc-number">2.1.</span> <span class="toc-text">1、整体架构如下：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、工程文件介绍"><span class="toc-number">2.2.</span> <span class="toc-text">2、工程文件介绍</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、新浪新闻爬虫"><span class="toc-number">3.</span> <span class="toc-text">三、新浪新闻爬虫</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=2428685001&verifier=553ef0af&dpc=1"></iframe>
</div>


  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/HPC/" title="HPC">HPC<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/Machine-Learning/" title="Machine Learning">Machine Learning<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/android/" title="android">android<sup>6</sup></a></li>
		  
		
		  
		
		  
			<li><a href="/categories/database/" title="database">database<sup>1</sup></a></li>
		  
		
		  
		
		  
		
		  
		
		  
		
		  
			<li><a href="/categories/django/" title="django">django<sup>1</sup></a></li>
		  
		
		  
		
		  
		
		  
		
		  
			<li><a href="/categories/life/" title="life">life<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/linux/" title="linux">linux<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/photo/" title="photo">photo<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/python/" title="python">python<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/前端/" title="前端">前端<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/生活/" title="生活">生活<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/茶语/" title="茶语">茶语<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">归档</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">三月 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">二月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">一月 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">十一月 2014</a><span class="archive-list-count">9</span></li></ul>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://blog.csdn.net/gugugujiawei" target="_blank" title="enjoyhot&#39;s CSDN">enjoyhot&#39;s CSDN</a>
            
          </li>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.enjoyhot.top" target="_blank" title="其他域名">其他域名</a>
            
          </li>
        
    </ul>
</div>

  
  <div class="tagcloudlist">
    <p class="asidetitle">标签云</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/HPC/" style="font-size: 12.5px;">HPC</a><a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a><a href="/tags/android/" style="font-size: 20px;">android</a><a href="/tags/cluster/" style="font-size: 10px;">cluster</a><a href="/tags/django/" style="font-size: 10px;">django</a><a href="/tags/html/" style="font-size: 10px;">html</a><a href="/tags/javascript/" style="font-size: 10px;">javascript</a><a href="/tags/life/" style="font-size: 12.5px;">life</a><a href="/tags/linux/" style="font-size: 15px;">linux</a><a href="/tags/listview/" style="font-size: 10px;">listview</a><a href="/tags/markdown/" style="font-size: 10px;">markdown</a><a href="/tags/mongoengine/" style="font-size: 10px;">mongoengine</a><a href="/tags/ndk/" style="font-size: 12.5px;">ndk</a><a href="/tags/photo/" style="font-size: 10px;">photo</a><a href="/tags/pymongo/" style="font-size: 10px;">pymongo</a><a href="/tags/python/" style="font-size: 17.5px;">python</a><a href="/tags/scrapy/" style="font-size: 10px;">scrapy</a><a href="/tags/前端/" style="font-size: 10px;">前端</a><a href="/tags/实验室/" style="font-size: 10px;">实验室</a><a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a><a href="/tags/生活/" style="font-size: 10px;">生活</a><a href="/tags/茶语/" style="font-size: 10px;">茶语</a>
    </div>
  </div>


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>


<br>
<!-- 打赏表单 -->
<form id="donate" action="https://shenghuo.alipay.com/send/payment/fill.htm" 
	method="POST" target="_blank" accept-charset="GBK" style="display: none;">
	<input name="optEmail" type="hidden" value="13570236302" />
	<input name="payAmount" type="hidden" value="1" />
	<input id="title" name="title" type="hidden" value="默认显示的付款说明" />
	<input name="memo" type="hidden" value="备注" />
</form>
<!-- /打赏表单 -->

<!-- 打赏按钮的样式表 -->
<style type="text/css">
	.donate_bar{ text-align: center; }
	.donate_bar a.btn_donate{
		display: inline-block;
		width: 82px;
		height: 82px;
		background: url("http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif") no-repeat;
		_background: url("http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif") no-repeat; 
	}
	.donate_bar a.btn_donate:hover{ background-position: 0px -82px;}
	.donate_bar .donate_txt {
		display: block;
		color: #9d9d9d;
		font: 14px/2 "Microsoft Yahei";
	}
</style>
<!-- /打赏按钮的样式表 -->

<!-- 打赏按钮 -->
<div class="donate_bar">
	<a class="btn_donate" href="javascript:;" title="Donate 打赏" 
		onclick="document.getElementById('donate').submit()"></a>
	<span class="donate_txt">
		&uarr;<br/>
		If you enjoy the blog,
		please feel free to donate~
		Thx for your support.
	</span>
	<span class="donate_txt">
		若本文对您有帮助，求打赏~ 谢谢您的支持和鼓励。
	</span>
</div>
<!-- /打赏按钮 -->
</div>

    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Jiawei Gu in CCNL,SCUT. <br/>
			Here shares some of my work about tech,life and study.Welcome to communicate and discuss together.^_^</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/2428685001" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/enjoyhot" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		<a href="https://www.linkedin.com/in/gugugujiawei" target="_blank" class="icon-linkedin" title="linkedin"></a>
		
		
		
		<a href="https://www.zhihu.com/people/enjoyhot" target="_blank" class="icon-zhihu" title="知乎"></a>
		
		
		<a href="https://plus.google.com/111736291216286617217?rel=author" target="_blank" class="icon-google_plus" title="Google+"></a>
		
		
		<a href="mailto:gugugujiawei@gmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2015 
		
		<a href="http://www.enjoyhot.top/about" target="_blank" title="enjoyhot">enjoyhot</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
  
    c.click();
    
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"enjoyhot"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 







<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F715efc417137bb8b58f6b4edfa378ccd' type='text/javascript'%3E%3C/script%3E"));
</script>



<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254644815'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s23.cnzz.com/z_stat.php%3Fid%3D1254644815' type='text/javascript'%3E%3C/script%3E"));</script>

<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->


  </body>
</html>
